# 运行spider爬虫文件
scrapy runspider stackoverflow_spider.py -o abc.csv

# 新建一个scrapy项目
scrapy startproject myfirstscrapy

# 编写爬虫
cd myfirstscrapy
scrapy genspider dmoz_spider dmoz.org
vi myfirstscrapy/spider/dmoz_spider

# 运行爬虫
scrapy crawl dmoz_spider --nolog
    --nolog      # 不输出日志
    --loglevel=  # 指定日志的级别
    --logfile=   # 将日志保存到文件
    --output=    # 将获取的数据输出到文件
    --output-format=
    -s JOBDIR=crawl/somespider-1
# 列出有多少爬虫
scrapy list



scrapy命令:
scrapy --help
scrapy version
scrapy version -v
scrapy view 
